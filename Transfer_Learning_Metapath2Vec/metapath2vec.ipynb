{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc794e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import stellargraph\n",
    "from stellargraph import StellarGraph \n",
    "import multiprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27dcb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle.load(open('raw/DBLP/transductive_dataset.pkl', 'rb'))\n",
    "docID_venue = pickle.load(open('raw/DBLP/documentID_venue.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d17d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_label = pd.DataFrame(columns=['author_id', 'label', 'author_name'])\n",
    "paper_author = pd.DataFrame(columns=['paper_id', 'author_id'])\n",
    "paper_conf = pd.DataFrame(columns=['paper_id', 'conf_id'])\n",
    "paper_term = pd.DataFrame(columns=['paper_id', 'term_id'])\n",
    "papers = pd.DataFrame(columns=['paper_id', 'paper_title'])\n",
    "terms = pd.DataFrame(columns=['term_id', 'term'])\n",
    "confs = pd.DataFrame(columns=['conf_id', 'conf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae5791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "publication_list = ['sigmod', 'vldb', 'icde', 'icdt', 'edbt', 'pods', 'kdd', 'www',\n",
    "                      'sdm', 'pkdd', 'icdm', 'cikm', 'aaai', 'icml', 'ecml', 'colt',\n",
    "                      'uai', 'soda', 'focs', 'stoc', 'stacs']\n",
    "\n",
    "for i, record in enumerate(docID_venue):\n",
    "    venue = record[1]\n",
    "    for pub in publication_list:\n",
    "        if pub in venue.lower():\n",
    "            docID_venue[i][1] = pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da9fa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_counter = Counter()\n",
    "terms_counter = Counter()\n",
    "for record in dataset:\n",
    "    paper_id = record[0]\n",
    "    skillIdx = record[1].todense().nonzero()[1]\n",
    "    terms_counter.update(skillIdx)\n",
    "    authorIdx = record[2].todense().nonzero()[1]\n",
    "    authors_counter.update(authorIdx)\n",
    "    \n",
    "    for authorId in authorIdx:\n",
    "        paper_author = paper_author.append({'paper_id': paper_id, 'author_id': authorId}, ignore_index=True)\n",
    "    \n",
    "    for skillId in skillIdx:\n",
    "        paper_term = paper_term.append({'paper_id': paper_id, 'term_id': skillId}, ignore_index=True)\n",
    "        \n",
    "    papers = papers.append({'paper_id': paper_id, 'paper_title': 'na'}, ignore_index=True)\n",
    "\n",
    "unique_authors_idx = list(authors_counter.keys())\n",
    "for unique_authors_id in unique_authors_idx:\n",
    "    author_label = author_label.append({'author_id': unique_authors_id, 'label': -1, 'author_name': 'na'}, ignore_index=True)\n",
    "    \n",
    "unique_terms_idx = list(terms_counter.keys())\n",
    "for unique_terms_id in unique_terms_idx:\n",
    "    terms = terms.append({'term_id': unique_terms_id, 'term': 'na'}, ignore_index=True)\n",
    "    \n",
    "conf_counter = Counter()\n",
    "for record in docID_venue:\n",
    "    paper_id = record[0]\n",
    "    conf_counter.update([record[1]])\n",
    "venues = list(conf_counter.keys())\n",
    "\n",
    "conf_confID = {}\n",
    "for i, venue in enumerate(venues):\n",
    "    confs = confs.append({'conf_id': i, 'conf': venue}, ignore_index=True)\n",
    "    conf_confID.update({venue: i})\n",
    "    \n",
    "for record in docID_venue:\n",
    "    paper_id = record[0]\n",
    "    conf_id = conf_confID[record[1]]\n",
    "    paper_conf = paper_conf.append({'paper_id': paper_id, 'conf_id': conf_id}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee6d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_counter = Counter()\n",
    "terms_counter = Counter()\n",
    "author_size_counter = Counter()\n",
    "for record in dataset:\n",
    "    paper_id = record[0]\n",
    "    skillIdx = record[1].todense().nonzero()[1]\n",
    "    terms_counter.update(skillIdx)\n",
    "    authorIdx = record[2].todense().nonzero()[1]\n",
    "    authors_counter.update(authorIdx)\n",
    "    author_size_counter.update([record[2].todense().shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b9d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = author_label['author_id'].to_list()\n",
    "paper_author = paper_author[paper_author['author_id'].isin(authors)].reset_index(drop=True)\n",
    "valid_papers = paper_author['paper_id'].unique()\n",
    "print('Number of papers :', len(valid_papers))\n",
    "\n",
    "papers = papers[papers['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "paper_conf = paper_conf[paper_conf['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "print('Number of papers :', len(paper_conf))\n",
    "\n",
    "paper_term = paper_term[paper_term['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "valid_terms = paper_term['term_id'].unique()\n",
    "terms = terms[terms['term_id'].isin(valid_terms)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "author_label = author_label.sort_values('author_id').reset_index(drop=True)\n",
    "papers = papers.sort_values('paper_id').reset_index(drop=True)\n",
    "terms = terms.sort_values('term_id').reset_index(drop=True)\n",
    "confs = confs.sort_values('conf_id').reset_index(drop=True)\n",
    "\n",
    "print('Number of conferences ', len(confs))\n",
    "print('Number of authors ', len(author_label))\n",
    "print('Number of terms ', len(terms))\n",
    "print('Number of papers ', len(papers))\n",
    "\n",
    "authors_list = list(author_label['author_id'])\n",
    "papers_list = list(papers['paper_id'])\n",
    "term_list = list(terms['term_id'])\n",
    "conf_list = list(confs['conf_id'])\n",
    "dim = len(authors_list) + len(papers_list) + len(term_list) + len(confs)\n",
    "print(' Total entities :: ', dim)\n",
    "\n",
    "\n",
    "author_id_mapping = {row['author_id']: i for i, row in author_label.iterrows()}\n",
    "paper_id_mapping = {row['paper_id']: i + len(author_label) for i, row in papers.iterrows()}\n",
    "term_id_mapping = {row['term_id']: i + len(author_label) + len(papers) for i, row in terms.iterrows()}\n",
    "conf_id_mapping = {row['conf_id']: i + len(author_label) + len(papers) + len(terms) for i, row in confs.iterrows()}\n",
    "\n",
    "\n",
    "entity_id_map = pd.DataFrame(\n",
    "    columns=['domain', 'entity_id','serial_id']\n",
    ")\n",
    "type_dict = { 'author': author_id_mapping, 'paper': paper_id_mapping, 'term': term_id_mapping, 'conf': conf_id_mapping }\n",
    "for _type,_dict in type_dict.items():\n",
    "    i = list(_dict.keys())\n",
    "    j = list(_dict.values())\n",
    "    _df = pd.DataFrame( data = {'entity_id': i ,'serial_id': j } )\n",
    "    _df['domain'] = _type\n",
    "    entity_id_map = entity_id_map.append(_df, ignore_index=True)\n",
    "\n",
    "    \n",
    "# ======================================================\n",
    "# Save data\n",
    "# ======================================================\n",
    "data_save_path = 'processed_data_metapath2vec/'\n",
    "if not os.path.exists('processed_data_metapath2vec'):\n",
    "    os.mkdir('processed_data_metapath2vec')\n",
    "if not os.path.exists(data_save_path):\n",
    "    os.mkdir(data_save_path)\n",
    "entity_id_map.to_csv( os.path.join( data_save_path, 'entity_id_mapping.csv') ) \n",
    "\n",
    "# Create graph data\n",
    "nodes_author_df = pd.DataFrame( data = { 'author' : list(author_id_mapping.values()) })\n",
    "nodes_paper_df = pd.DataFrame(  data = { 'paper' : list(paper_id_mapping.values()) } )\n",
    "nodes_term_df = pd.DataFrame( data = { 'term' : list(term_id_mapping.values()) } )\n",
    "nodes_conf_df = pd.DataFrame(  data = { 'conf' : list(conf_id_mapping.values()) } )\n",
    "\n",
    "nodes_author_df.to_csv(os.path.join(data_save_path,'nodes_author.csv'),index = False)\n",
    "nodes_paper_df.to_csv(os.path.join(data_save_path,'nodes_paper.csv'),index = False)\n",
    "nodes_term_df.to_csv(os.path.join(data_save_path,'nodes_term.csv'),index = False)\n",
    "nodes_conf_df.to_csv(os.path.join(data_save_path,'nodes_conf.csv'),index = False)\n",
    "\n",
    "PA_edge_list = []\n",
    "for _, row in paper_author.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = author_id_mapping[row['author_id']]\n",
    "    PA_edge_list.append((idx1,idx2))\n",
    "    \n",
    "df = pd.DataFrame ( data =  np.array(PA_edge_list), columns = ['source','target'])\n",
    "fpath = os.path.join(data_save_path, 'PA_edges.csv')\n",
    "df.to_csv(fpath, index=False)\n",
    "    \n",
    "PT_edge_list = []\n",
    "for _, row in paper_term.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = term_id_mapping[row['term_id']]\n",
    "    PT_edge_list.append((idx1,idx2))\n",
    "\n",
    "df = pd.DataFrame ( data =  np.array(PT_edge_list), columns = ['source','target'])\n",
    "fpath = os.path.join(data_save_path, 'PT_edges.csv')\n",
    "df.to_csv(fpath, index=False)\n",
    "    \n",
    "\n",
    "PC_edge_list = []\n",
    "for _, row in paper_conf.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = conf_id_mapping[row['conf_id']]\n",
    "    PC_edge_list.append((idx1,idx2))\n",
    "\n",
    "df = pd.DataFrame ( data = np.array(PC_edge_list), columns = ['source','target'])\n",
    "fpath = os.path.join(data_save_path, 'PC_edges.csv')\n",
    "df.to_csv(fpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdebb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_counter = Counter()\n",
    "tp_counter = Counter()\n",
    "pc_counter = Counter()\n",
    "\n",
    "for i, _ in PA_edge_list:\n",
    "    ap_counter.update([i])\n",
    "print(np.mean(list(ap_counter.values())))\n",
    "\n",
    "for i, _ in PT_edge_list:\n",
    "    tp_counter.update([i])\n",
    "print(np.mean(list(tp_counter.values())))\n",
    "\n",
    "for _, i in PC_edge_list:\n",
    "    pc_counter.update([i])\n",
    "print(np.mean(list(pc_counter.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25015d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Create data for HIN2Vec\n",
    "# ==============================\n",
    "\n",
    "df = pd.DataFrame(columns=['node1', 'node2','rel'])\n",
    "for edge in PA_edge_list:\n",
    "    df = df.append({'node1':edge[0],'node2':edge[1],'rel': 0},ignore_index=True )\n",
    "\n",
    "for edge in PT_edge_list:\n",
    "    df = df.append({'node1':edge[0],'node2':edge[1],'rel': 1},ignore_index=True )\n",
    "    \n",
    "for edge in PC_edge_list:\n",
    "    df = df.append({'node1':edge[0],'node2':edge[1],'rel': 2},ignore_index=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19e51d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['node1'] = df['node1'].astype(int)\n",
    "df['node2'] = df['node2'].astype(int)\n",
    "df['rel'] = df['rel'].astype(int)\n",
    "fpath = os.path.join(data_save_path,'hin2vec_dblp_input.txt')\n",
    "df.to_csv( fpath, index = None, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff22999",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = './processed_data_metapath2vec/'\n",
    "\n",
    "nodes_author_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        src_dir,\n",
    "        'nodes_author.csv'),\n",
    "    index_col = 0\n",
    ")\n",
    "nodes_paper_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        src_dir,\n",
    "        'nodes_paper.csv'),\n",
    "    index_col = 0\n",
    ")\n",
    "nodes_term_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        src_dir,\n",
    "        'nodes_term.csv'),\n",
    "    index_col = 0\n",
    ")\n",
    "nodes_conf_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        src_dir,\n",
    "        'nodes_conf.csv'),\n",
    "    index_col = 0\n",
    ")\n",
    "\n",
    "fpath_list = ['PT_edges.csv','PC_edges.csv','PA_edges.csv']\n",
    "df_edges = None\n",
    "for fpath in fpath_list:\n",
    "    _df = pd.read_csv( os.path.join(src_dir,fpath), index_col = None )\n",
    "    if df_edges is None : df_edges = _df\n",
    "    else:\n",
    "        df_edges = df_edges.append(_df,ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_obj = StellarGraph({\n",
    "    \"author\": nodes_author_df,\n",
    "    \"paper\": nodes_paper_df,\n",
    "    \"term\": nodes_term_df,\n",
    "    \"conf\": nodes_conf_df\n",
    "},\n",
    "    df_edges\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ea0874",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_obj.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_use_data_DIR = 'model_use_data'\n",
    "if not os.path.exists(model_use_data_DIR):\n",
    "    os.mkdir(model_use_data_DIR)\n",
    "model_use_data_DIR = os.path.join(model_use_data_DIR,'DBLP')\n",
    "if not os.path.exists(model_use_data_DIR):\n",
    "    os.mkdir(model_use_data_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e154e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_length = 64  # maximum length of a random walk to use throughout this notebook\n",
    "\n",
    "# specify the metapath schemas as a list of lists of node types.\n",
    "metapaths = [\n",
    "    [\"author\", \"paper\", \"author\"],\n",
    "    [\"author\", \"paper\", \"term\", \"paper\", \"author\"],\n",
    "    [\"author\", \"paper\", \"conf\", \"paper\", \"author\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6dbaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.data import UniformRandomMetaPathWalk\n",
    "\n",
    "# Create the random walker\n",
    "rw = UniformRandomMetaPathWalk(graph_obj)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "walks = rw.run(\n",
    "    nodes=list(graph_obj.nodes()),  # root nodes\n",
    "    length=walk_length,  # maximum length of a random walk\n",
    "    n=40,  # number of random walks per root node\n",
    "    metapaths=metapaths,  # the metapaths\n",
    ")\n",
    "end = time.time()\n",
    "print(\"Time taken: \", end - start)\n",
    "print(\"Number of random walks: {}\".format(len(walks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67846936",
   "metadata": {},
   "outputs": [],
   "source": [
    "for walk in walks:\n",
    "    for i in range(len(walk)):\n",
    "        walk[i] = str(walk[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f80782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import time\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "        \n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "class callback(CallbackAny2Vec):\n",
    "    '''Callback to print loss after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00367d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "time_callback = TimeHistory()\n",
    "model = Word2Vec(walks, size=100, window=5, min_count=0, sg=1, workers=multiprocessing.cpu_count(), negative=1, compute_loss=True, iter=100, callbacks=[callback(), time_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f02d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = time_callback.times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74365ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_times = times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e95ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cumulative_times)):\n",
    "    if(i>0):\n",
    "        cumulative_times[i] = cumulative_times[i]+cumulative_times[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dac34ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_per_epoch = []\n",
    "for i in range(1,101):\n",
    "    time_per_epoch.append((i,cumulative_times[i-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_per_epoch = pd.DataFrame(time_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af2fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_per_epoch.to_csv(\"metapath2vec_time_per_epoch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf36c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embeddings = []\n",
    "model_save_path = 'model_save_dir'\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.mkdir(model_save_path)\n",
    "model_save_path = os.path.join(model_save_path,'DBLP')\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.mkdir(model_save_path)\n",
    "emb_fpath = os.path.join(model_save_path, 'mp2v_{}_{}_{}.npy'.format(128,40,100))\n",
    "np.save(emb_fpath, node_embeddings )\n",
    "\n",
    "# ======== Save node weights ============ #\n",
    "for i in range(len(graph_obj.nodes())):\n",
    "    vec = model.wv[str(i)]\n",
    "    node_embeddings.append(vec)\n",
    "node_embeddings = np.array(node_embeddings)\n",
    "np.save(emb_fpath, node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68941d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_id_mapping = pd.read_csv(src_dir + 'entity_id_mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d3676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embeddings_normalized = normalize(node_embeddings, axis=1, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b73f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {'user': {}, 'skill': {}}\n",
    "for i, row in entity_id_mapping.iterrows():\n",
    "    if row['domain'] == 'author':\n",
    "        embedding_dict['user'].update({row['entity_id']: node_embeddings_normalized[row['serial_id']]})  \n",
    "    elif row['domain'] == 'term':\n",
    "        embedding_dict['skill'].update({row['entity_id']: node_embeddings_normalized[row['serial_id']]}) \n",
    "pickle.dump(embedding_dict, open(model_save_path + '/embedding_dict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac162a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
