{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f297e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ef4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph import StellarGraph\n",
    "from stellargraph.mapper import (\n",
    "    CorruptedGenerator,\n",
    "    FullBatchNodeGenerator,\n",
    "    GraphSAGENodeGenerator,\n",
    "    HinSAGENodeGenerator,\n",
    "    HinSAGELinkGenerator,\n",
    "    ClusterNodeGenerator,\n",
    "    RelationalFullBatchNodeGenerator\n",
    ")\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.layer import DeepGraphInfomax, GAT, GCN, RGCN, HinSAGE\n",
    "from stellargraph.utils import plot_history\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ed363",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle.load(open('raw/DBLP/dblp_preprocessed_dataset.pkl', 'rb'))\n",
    "docID_venue = pickle.load(open('raw/DBLP/documentID_venue.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe4e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_label = pd.DataFrame(columns=['author_id', 'label', 'author_name', 'features'])\n",
    "paper_author = pd.DataFrame(columns=['paper_id', 'author_id'])\n",
    "paper_conf = pd.DataFrame(columns=['paper_id', 'conf_id'])\n",
    "paper_term = pd.DataFrame(columns=['paper_id', 'term_id'])\n",
    "papers = pd.DataFrame(columns=['paper_id', 'paper_title', 'features'])\n",
    "terms = pd.DataFrame(columns=['term_id', 'term', 'features'])\n",
    "confs = pd.DataFrame(columns=['conf_id', 'conf', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badae417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning venue names\n",
    "publication_list = ['sigmod', 'vldb', 'icde', 'icdt', 'edbt', 'pods', 'kdd', 'www',\n",
    "                      'sdm', 'pkdd', 'icdm', 'cikm', 'aaai', 'icml', 'ecml', 'colt',\n",
    "                      'uai', 'soda', 'focs', 'stoc', 'stacs']\n",
    "\n",
    "\n",
    "for i, record in enumerate(docID_venue):\n",
    "    venue = record[1]\n",
    "    for pub in publication_list:\n",
    "        if pub in venue.lower():\n",
    "            docID_venue[i][1] = pub\n",
    "\n",
    "docID_venue_dict = {}\n",
    "for record in docID_venue:\n",
    "    docID_venue_dict.update({record[0]:record[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4475fe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_counter = Counter()\n",
    "terms_counter = Counter()\n",
    "#authorID_feature = {}\n",
    "skillID_feature = {}\n",
    "venue_feature = {}\n",
    "for record in dataset:\n",
    "    paper_id = record[0]\n",
    "    skillVector = record[1].todense()\n",
    "    skillIdx = skillVector.nonzero()[1]\n",
    "    terms_counter.update(skillIdx)\n",
    "    authorVector = record[2].todense()\n",
    "    authorIdx = record[2].todense().nonzero()[1]\n",
    "    authors_counter.update(authorIdx)\n",
    "    \n",
    "    for authorId in authorIdx:\n",
    "        paper_author = paper_author.append({'paper_id': paper_id, 'author_id': authorId}, ignore_index=True)\n",
    "        \n",
    "#         if authorId not in authorID_feature.keys():\n",
    "#             authorID_feature.update({authorId: []})\n",
    "#         authorID_feature[authorId].append(skillVector)\n",
    "    \n",
    "    for skillId in skillIdx:\n",
    "        paper_term = paper_term.append({'paper_id': paper_id, 'term_id': skillId}, ignore_index=True)\n",
    "        \n",
    "        if skillId not in skillID_feature.keys():\n",
    "            skillID_feature.update({skillId: []})\n",
    "        skillID_feature[skillId].append(authorVector)\n",
    "        \n",
    "    papers = papers.append({'paper_id': paper_id, 'paper_title': 'na', 'features': authorVector}, ignore_index=True)\n",
    "    \n",
    "    target_venue = docID_venue_dict[paper_id]\n",
    "    if target_venue not in venue_feature.keys():\n",
    "        venue_feature.update({target_venue: []})\n",
    "    venue_feature[target_venue].append(authorVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e81d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_authors_idx = list(authors_counter.keys())\n",
    "for unique_authors_id in unique_authors_idx:\n",
    "    author_label = author_label.append({'author_id': unique_authors_id, 'label': -1, 'author_name': 'na'}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ad67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_terms_idx = list(terms_counter.keys())\n",
    "for unique_terms_id in unique_terms_idx:\n",
    "    if len(skillID_feature[unique_terms_id]) == 1:\n",
    "        term_features = skillID_feature[unique_terms_id]\n",
    "    else:\n",
    "        term_features = np.sum(skillID_feature[unique_terms_id], axis=0)\n",
    "    terms = terms.append({'term_id': unique_terms_id, 'term': 'na', 'features': term_features}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_counter = Counter()\n",
    "for record in docID_venue:\n",
    "    paper_id = record[0]\n",
    "    conf_counter.update([record[1]])\n",
    "venues = list(conf_counter.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf6284",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_confID = {}\n",
    "for i, venue in enumerate(venues):\n",
    "    if len(venue_feature[venue]) == 1:\n",
    "        venue_features = venueID_feature[i]\n",
    "    else:\n",
    "        venue_features = np.sum(venue_feature[venue], axis=0)\n",
    "    confs = confs.append({'conf_id': i, 'conf': venue, 'features': venue_features}, ignore_index=True)\n",
    "    conf_confID.update({venue: i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ad4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in docID_venue:\n",
    "    paper_id = record[0]\n",
    "    conf_id = conf_confID[record[1]]\n",
    "    paper_conf = paper_conf.append({'paper_id': paper_id, 'conf_id': conf_id}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c54191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_counter = Counter()\n",
    "terms_counter = Counter()\n",
    "for record in dataset:\n",
    "    paper_id = record[0]\n",
    "    skillIdx = record[1].todense().nonzero()[1]\n",
    "    terms_counter.update(skillIdx)\n",
    "    authorIdx = record[2].todense().nonzero()[1]\n",
    "    authors_counter.update(authorIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f737eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = author_label['author_id'].to_list()\n",
    "paper_author = paper_author[paper_author['author_id'].isin(authors)].reset_index(drop=True)\n",
    "valid_papers = paper_author['paper_id'].unique()\n",
    "print('Number of papers :', len(valid_papers))\n",
    "\n",
    "papers = papers[papers['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "paper_conf = paper_conf[paper_conf['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "print('Number of papers :', len(paper_conf))\n",
    "\n",
    "paper_term = paper_term[paper_term['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "valid_terms = paper_term['term_id'].unique()\n",
    "terms = terms[terms['term_id'].isin(valid_terms)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "author_label = author_label.sort_values('author_id').reset_index(drop=True)\n",
    "papers = papers.sort_values('paper_id').reset_index(drop=True)\n",
    "terms = terms.sort_values('term_id').reset_index(drop=True)\n",
    "confs = confs.sort_values('conf_id').reset_index(drop=True)\n",
    "\n",
    "print('Number of conferences ', len(confs))\n",
    "print('Number of authors ', len(author_label))\n",
    "print('Number of terms ', len(terms))\n",
    "print('Number of papers ', len(papers))\n",
    "\n",
    "authors_list = list(author_label['author_id'])\n",
    "papers_list = list(papers['paper_id'])\n",
    "term_list = list(terms['term_id'])\n",
    "conf_list = list(confs['conf_id'])\n",
    "dim = len(authors_list) + len(papers_list) + len(term_list) + len(confs)\n",
    "print(' Total entities :: ', dim)\n",
    "\n",
    "\n",
    "author_id_mapping = {row['author_id']: i for i, row in author_label.iterrows()}\n",
    "paper_id_mapping = {row['paper_id']: i + len(author_label) for i, row in papers.iterrows()}\n",
    "term_id_mapping = {row['term_id']: i + len(author_label) + len(papers) for i, row in terms.iterrows()}\n",
    "conf_id_mapping = {row['conf_id']: i + len(author_label) + len(papers) + len(terms) for i, row in confs.iterrows()}\n",
    "\n",
    "\n",
    "entity_id_map = pd.DataFrame(\n",
    "    columns=['domain', 'entity_id','serial_id']\n",
    ")\n",
    "type_dict = { 'author': author_id_mapping, 'paper': paper_id_mapping, 'term': term_id_mapping, 'conf': conf_id_mapping }\n",
    "for _type,_dict in type_dict.items():\n",
    "    i = list(_dict.keys())\n",
    "    j = list(_dict.values())\n",
    "    _df = pd.DataFrame( data = {'entity_id': i ,'serial_id': j } )\n",
    "    _df['domain'] = _type\n",
    "    entity_id_map = entity_id_map.append(_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c877ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# Save data\n",
    "# ======================================================\n",
    "data_save_path = 'processed_data/'\n",
    "if not os.path.exists('processed_data'):\n",
    "    os.mkdir('processed_data')\n",
    "if not os.path.exists(data_save_path):\n",
    "    os.mkdir(data_save_path)\n",
    "entity_id_map.to_csv( os.path.join( data_save_path, 'entity_id_mapping.csv')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eedb01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph data\n",
    "nodes_author_df = pd.DataFrame({'feature1': [0]}, index=list(author_id_mapping.values()))\n",
    "nodes_paper_df = pd.DataFrame([np.asarray(row['features']).flatten() for i, row in papers.iterrows()], index=list(paper_id_mapping.values()))\n",
    "nodes_term_df = pd.DataFrame([np.asarray(row['features']).flatten() for i, row in terms.iterrows()], index=list(term_id_mapping.values()))\n",
    "nodes_conf_df = pd.DataFrame([np.asarray(row['features']).flatten() for i, row in confs.iterrows()], index=list(conf_id_mapping.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2032dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_author_df.to_csv(os.path.join(data_save_path,'nodes_author.csv'),index = False)\n",
    "nodes_paper_df.to_csv(os.path.join(data_save_path,'nodes_paper.csv'),index = False)\n",
    "nodes_term_df.to_csv(os.path.join(data_save_path,'nodes_term.csv'),index = False)\n",
    "nodes_conf_df.to_csv(os.path.join(data_save_path,'nodes_conf.csv'),index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba043f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA_edge_list = []\n",
    "for _, row in paper_author.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = author_id_mapping[row['author_id']]\n",
    "    PA_edge_list.append((idx1,idx2))\n",
    "    \n",
    "df = pd.DataFrame ( data =  np.array(PA_edge_list), columns = ['source','target'])\n",
    "fpath = os.path.join(data_save_path, 'PA_edges.csv')\n",
    "df.to_csv(fpath, index=False)\n",
    "    \n",
    "PT_edge_list = []\n",
    "for _, row in paper_term.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = term_id_mapping[row['term_id']]\n",
    "    PT_edge_list.append((idx1,idx2))\n",
    "\n",
    "df = pd.DataFrame ( data =  np.array(PT_edge_list), columns = ['source','target'])\n",
    "fpath = os.path.join(data_save_path, 'PT_edges.csv')\n",
    "df.to_csv(fpath, index=False)\n",
    "    \n",
    "\n",
    "PC_edge_list = []\n",
    "for _, row in paper_conf.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = conf_id_mapping[row['conf_id']]\n",
    "    PC_edge_list.append((idx1,idx2))\n",
    "\n",
    "df = pd.DataFrame ( data = np.array(PC_edge_list), columns = ['source','target'])\n",
    "fpath = os.path.join(data_save_path, 'PC_edges.csv')\n",
    "df.to_csv(fpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9224698",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Saving Expert Node Features #################\n",
    "te = np.zeros((2076,2076))\n",
    "for i in range(len(te)):\n",
    "    for j in range(len(te[i])):\n",
    "        if(i==j):\n",
    "            te[i][j]=1\n",
    "\n",
    "df_te = pd.DataFrame(te)\n",
    "df_te.to_csv(\"Author_IdentityMatrix_Features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb261d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Saving Expert Node Features #################\n",
    "te = np.zeros((2470,2470))\n",
    "for i in range(len(te)):\n",
    "    for j in range(len(te[i])):\n",
    "        if(i==j):\n",
    "            te[i][j]=1\n",
    "\n",
    "df_te = pd.DataFrame(te)\n",
    "df_te.to_csv(\"Author_IdentityMatrix_Features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d027f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = './processed_data/'\n",
    "\n",
    "nodes_DBLP_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        src_dir,\n",
    "        'nodes_DBLP.csv'),\n",
    "    index_col = 0\n",
    ")\n",
    "\n",
    "\n",
    "fpath_list = ['nodes_DBLP_edges_withRelations.csv']\n",
    "df_edges = None\n",
    "for fpath in fpath_list:\n",
    "    _df = pd.read_csv( os.path.join(src_dir,fpath), index_col = None )\n",
    "    if df_edges is None : df_edges = _df\n",
    "    else:\n",
    "        df_edges = df_edges.append(_df,ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab0ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_obj = StellarGraph({\n",
    "    \"DBLP\": nodes_DBLP_df\n",
    "},\n",
    "    df_edges, edge_type_column=\"orientation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd93455",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_obj.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e444d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
